# Exploring-Unlabeled-Speaker-Recognition

> **Refer to the full documentation (`Exploring Unlabeled Speaker Recognition Documentation.pdf`) if you need more detailed explanations.**

---

## Overview

This repository proposes and outlines a **fullyâ€‘unsupervised, highâ€‘accuracy pipeline** for identifying *â‰ˆ200* unique speakers in an unlabeled collection of WAV recordings.
The core idea:

1. **Preâ€‘process** audio (resample, VAD, normalise).
2. **Embed** each utterance with a **preâ€‘trained ECAPAâ€‘TDNN** (or xâ€‘vector) network.
3. **Cluster** the embedding vectors (HDBSCANâ€¯/â€¯Spectral Clustering) to form speaker IDs.
4. **Validate** clusters using internal metrics & manual spotâ€‘checks.

Most of the heavy lifting is done by the powerful speakerâ€‘embedding model, allowing accurate separation without labelled data.

---

## ğŸ” Data Exploration & Analysis

| **Exploration Step**                 | **Purpose**                                                                                         |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- |
| Listening to audio samples           | Understand voice diversity, accents, noise, and potential overlap between speakers                  |
| Plotting waveform diagrams           | Identify amplitude variations, silence regions, clipping, or speech rhythm anomalies                |
| Spectrogram visualization            | Observe pitch, formant structures, and frequency distribution across time                           |
| Statistical feature extraction       | Analyze MFCCs, pitch, and speaking rate to detect broad groupings (e.g., by gender or vocal traits) |
| Silence and speech segmentation      | Detect and isolate active voice regions using VAD; flag recordings with multiple speakers           |
| Feature space projection (PCA/t-SNE) | Visualize embedding clusters to assess speaker separability and clustering potential                |


Challenges discovered:

* Varying recording quality & background noise.
* Possible nearâ€‘identical voices.
* No groundâ€‘truth labels for evaluation.

---

## ğŸ’¡ Proposed Solution & Justification

| **Component**            | **Selected Method**                                                     | **Justification**                                                                       |
| ------------------------ | ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Speaker Embeddings**   | ECAPAâ€‘TDNN (192â€‘D)                                                      | State-of-the-art model with strong robustness to noise and speaker variability          |
| **Similarity Metric**    | Cosine similarity                                                       | Scale-invariant; matches the training objective of modern speaker embeddings            |
| **Clustering Algorithm** | HDBSCAN                                          | Handles variable cluster sizes; identifies outliers; no need to predefine cluster count |
| **Evaluation Strategy**  | Silhouette score, Davies-Bouldin Index (DBI), manual audio verification | Effective for unsupervised validation when no ground truth is available                 |


Why it works: embeddings compress speaker identity into a compact vector; clustering then groups vectors that are naturally close in this space.

---

## ğŸ› ï¸ Conceptual Implementation Strategy

```text
src/
â”œâ”€â”€ preprocess.py         # 1.  Audio cleaning
â”‚   â€¢ resample to 16â€¯kHz mono
â”‚   â€¢ apply VAD â†’ trim silence / noise
â”‚   â€¢ loudnessâ€‘normalise â†’ data/clean/
â”‚
â”œâ”€â”€ embed.py              # 2.  Speakerâ€‘vector extraction
â”‚   â€¢ load SpeechBrain ECAPAâ€‘TDNN
â”‚   â€¢ each WAV â†’ 192â€‘D embedding
â”‚   â€¢ stack â†’ embeddings.npy  +  filenames.txt
â”‚
â”œâ”€â”€ cluster.py            # 3.  Unsupervised grouping
â”‚   â€¢ load embeddings.npy  (L2â€‘normalised)
â”‚   â€¢ run HDBSCAN(min_cluster_size=2)
â”‚   â€¢ output cluster_map.json  {filename: cluster_id}
â”‚
â””â”€â”€ evaluate.py           # 4.  Quality checks
    â€¢ compute Silhouette & Daviesâ€‘Bouldin indices
    â€¢ plot clusterâ€‘size histogram
    â€¢ list recordings with low silhouette for manual review
```

---

## âš ï¸ Challenges & Mitigations

| Challenge                 | Mitigation                                                                          |
| ------------------------- | ----------------------------------------------------------------------------------- |
| Similarâ€‘sounding speakers | Highâ€‘resolution ECAPA embeddings; iterative reâ€‘clustering of suspect large clusters |
| Noise / channel mismatch  | VAD + light spectral denoising; robustness of ECAPA (trained with augmentation)     |
| No ground truth           | Internal metrics; targeted manual listening; verificationâ€‘score crossâ€‘checks        |
| Overâ€‘ / underâ€‘clustering  | Compare HDBSCAN vs. fixedâ€‘k AHC, inspect cluster size distribution                  |


---

## ğŸ—‚ï¸ Repository Layout

```
Exploring-Unlabeled-Speaker-Recognition/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # <-- original WAVs go here
â”‚   â”œâ”€â”€ combined/     # <-- concatenated WAVs go here (Optional)
â”‚   â”œâ”€â”€ clean/        # autoâ€‘generated VADâ€‘trimmed clips
â”‚   â””â”€â”€ embeddings/   # autoâ€‘generated .npy vectors
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_recordings.py # copy Audioâ€‘MNIST --> data/raw/
â”‚   â”œâ”€â”€ combine_audio.py # concatenate digit recordings --> data/combined/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ cluster.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ Exploring Unlabeled Speaker Recognition Documentation.pdf   # <-- Detailed writeâ€‘up
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---

## ğŸš€ Quickâ€‘Start

```bash
# 0) Clone repository
git clone https://github.com/Kai-Heng/Exploring-Unlabelled-Speaker-Recognition.git
cd Exploring-Unlabelled-Speaker-Recognition

# 1) Set up Python 3.12 environment
python3.12 -m venv AINgineer
source AINgineer/bin/activate          # (Windows â†’ AINgineer\Scripts\activate)
pip install -r requirements.txt

# 2) Prepare data  
# â€¢ If you already have WAVs in  data/raw/  â†’ **skip this step.**
# â€¢ Otherwise, populate it from the Audioâ€‘MNIST download (â‰ˆâ€¯30â€¯000 files):
python src/extract_recordings.py --src /path/to/AudioMNIST/data/
python src/combine_audio.py

# 2. Run pipeline
python src/preprocess.py   # cleans /data/*.wav â†’ /data/clean/*.wav
python src/embed.py        # â†’ embeddings.npy
python src/cluster.py      # â†’ cluster_map.json
python src/evaluate.py     # prints silhouette & saves plots
```

---

## ğŸ“Š Clustering Evaluation Results (Experiment with 60 speakers)

| Algorithm                                            | SilhouetteÂ Score<sup>â€ </sup> | Daviesâ€‘BouldinÂ Index<sup>â€¡</sup> | Observations                                                                                                       |
| ---------------------------------------------------- | ---------------------------- | -------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **HDBSCAN**                                          | **0.510**                    | **0.824**                        | Same overall quality as Kâ€‘Means on this dataset, while automatically discovering cluster count and flagging noise. |
| **Kâ€‘Means** (kâ€¯=â€¯60, n\_initâ€¯=â€¯20, seedâ€¯=â€¯42)        | **0.510**                    | **0.824**                        | Fast and simple; matched HDBSCANâ€™s scores but required preâ€‘setting *k* and forces every point into a cluster.      |
| **SpectralÂ Clustering** (nearestâ€‘neighbors affinity) | âˆ’0.024                       | 2.614                            | Performed poorly â€” negative silhouette and high DB index indicate illâ€‘formed clusters for this embedding.          |
| **SpectralÂ Clustering** (radial basis function) | **0.510**                       | **0.824**                            | Same overall quality as Kâ€‘Means on this dataset while using rdf to construct affinity matrix          |

<sup>â€ </sup> **Silhouette score** âˆˆâ€¯\[âˆ’1,â€¯1] Â (+1â€¯=â€¯wellâ€‘separated, Â 0â€¯â‰ˆâ€¯overlap, Â âˆ’1â€¯=â€¯misâ€‘clustered. Higher is better.)

<sup>â€¡</sup> **Daviesâ€‘Bouldin index**Â â‰¥â€¯0 Â (0â€¯=â€¯perfectly compact/isolated clusters. Lower is better.)


Complete JSON mapping of recordingâ€¯â†’â€¯cluster is in ```data/embeddings/cluster_map.json```.

---

### ğŸ“ˆ Metric Ranges & Why They Matter

* **Silhouette Score (S)**

  * Range **âˆ’1â€¯â†’â€¯1**.
  * *Interpretation*: Sâ€¯â‰³â€¯0.5 is generally considered good; Sâ€¯<â€¯0 suggests points are assigned to the wrong clusters.

* **Daviesâ€‘Bouldin Index (DBI)**

  * Range **0â€¯â†’â€¯âˆ**.
  * *Interpretation*: DBIâ€¯â‰²â€¯1 indicates compact, wellâ€‘separated clusters; valuesâ€¯â‰«â€¯1 mean high overlap.

These complementary metrics help avoid relying on a single view of cluster quality.

---

### Quick Takeaways

* **HDBSCAN** matched Kâ€‘Meansâ€™ quantitative scores **without** requiring you to guess *k* and **flagged noise points** automatically â€” valuable for speakerâ€‘embedding spaces that may contain outliers.
* **Spectral Clustering** underâ€‘performed on the same embeddings, suggesting either a poor affinity choice or that the speaker manifold is not well captured by a graphâ€‘based approach here.
* Given equal scores, you might prefer **HDBSCAN** for its flexibility and practical benefits (automatic cluster count, noise handling).

---

## ğŸ§ About the Dataset â€” *AudioMNIST (Combined)*

| Property                | Value                                       |
| ----------------------- | ------------------------------------------- |
| **Total clips**         | 30â€¯000 WAV files                            |
| **Speakers**            | 60 (one folder per speaker)                 |
| **Digits**              | 0â€¯â€“â€¯9 (spoken)                              |
| **Samples per speaker** | 500 (â‰ˆÂ 50 clips per digit before combining) |
| **Metadata**            | `audioMNIST_meta.txt` (gender, age, etc.)   |
| **Source**              | Kaggle dataset â€œAudio MNISTâ€        |

For this task, the raw digit recordings for each speaker were **concatenated across digits** â€” that is, each combined clip contains a sequence of digits from 0 to 9 spoken in order. This process was repeated 50 times per speaker, resulting in **60 speakers Ã— 50 combined recordings = 3â€¯000 utterances**. These longer clips were created to ensure greater phonetic variability per recording, enabling the CAPAâ€‘TDNN model to better capture speaker-specific features for clustering.

---

## ğŸ§  Future Work

While this pipeline demonstrates solid performance on a structured dataset like AudioMNIST, several areas remain open for extension and experimentation:

* **Generalization to real-world data:** Test on more varied datasets with spontaneous speech, background noise, and cross-channel recordings.
* **Dynamic clustering techniques:** Investigate semi-supervised refinement or pseudo-label bootstrapping using high-confidence clusters.
* **Incremental speaker addition:** Explore how the model handles unseen speakers and how embeddings generalize to new voices.
* **Embedding model fine-tuning:** Fine-tune ECAPA-TDNN on unlabeled target domain recordings using self-supervised objectives to better capture domain-specific features.
* **Post-processing with centroid modeling:** Use cluster centroids as speaker prototypes and apply few-shot classification for new recordings.

---
