# Exploring-Unlabelled-Speaker-Recognition

> **Refer to the full documentation (`Exploring Unlabelled Speaker Recognition Documentation.pdf`) if you need more detailed explanations.**

---

## Overview

This repository proposes and outlines a **fullyâ€‘unsupervised, highâ€‘accuracy pipeline** for identifying *â‰ˆ200* unique speakers in an unlabeled collection of WAV recordings.
The core idea:

1. **Preâ€‘process** audio (resample, VAD, normalise).
2. **Embed** each utterance with a **preâ€‘trained ECAPAâ€‘TDNN** (or xâ€‘vector) network.
3. **Cluster** the embedding vectors (HDBSCANâ€¯/â€¯Spectral Clustering) to form speaker IDs.
4. **Validate** clusters using internal metrics & manual spotâ€‘checks.

Most of the heavy lifting is done by the powerful speakerâ€‘embedding model, allowing accurate separation without labelled data.

---

## Data Exploration & Analysis

| **Exploration Step**                 | **Purpose**                                                                                         |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- |
| Listening to audio samples           | Understand voice diversity, accents, noise, and potential overlap between speakers                  |
| Plotting waveform diagrams           | Identify amplitude variations, silence regions, clipping, or speech rhythm anomalies                |
| Spectrogram visualization            | Observe pitch, formant structures, and frequency distribution across time                           |
| Statistical feature extraction       | Analyze MFCCs, pitch, and speaking rate to detect broad groupings (e.g., by gender or vocal traits) |
| Silence and speech segmentation      | Detect and isolate active voice regions using VAD; flag recordings with multiple speakers           |
| Feature space projection (PCA/t-SNE) | Visualize embedding clusters to assess speaker separability and clustering potential                |


Challenges discovered:

* Varying recording quality & background noise.
* Possible nearâ€‘identical voices.
* No groundâ€‘truth labels for evaluation.

---

## Proposed Solution & Justification

| **Component**            | **Selected Method**                                                     | **Justification**                                                                       |
| ------------------------ | ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Speaker Embeddings**   | ECAPAâ€‘TDNN (192â€‘D)                                                      | State-of-the-art model with strong robustness to noise and speaker variability          |
| **Similarity Metric**    | Cosine similarity                                                       | Scale-invariant; matches the training objective of modern speaker embeddings            |
| **Clustering Algorithm** | HDBSCAN                                          | Handles variable cluster sizes; identifies outliers; no need to predefine cluster count |
| **Evaluation Strategy**  | Silhouette score, Davies-Bouldin Index (DBI), manual audio verification | Effective for unsupervised validation when no ground truth is available                 |


Why it works: embeddings compress speaker identity into a compact vector; clustering then groups vectors that are naturally close in this space.

---

## Conceptual Implementation Strategy

```text
src/
â”œâ”€â”€ preprocess.py         # 1.  Audio cleaning
â”‚   â€¢ resample to 16â€¯kHz mono
â”‚   â€¢ apply VAD â†’ trim silence / noise
â”‚   â€¢ loudnessâ€‘normalise â†’ data/clean/
â”‚
â”œâ”€â”€ embed.py              # 2.  Speakerâ€‘vector extraction
â”‚   â€¢ load SpeechBrain ECAPAâ€‘TDNN
â”‚   â€¢ each WAV â†’ 192â€‘D embedding
â”‚   â€¢ stack â†’ embeddings.npy  +  filenames.txt
â”‚
â”œâ”€â”€ cluster.py            # 3.  Unsupervised grouping
â”‚   â€¢ load embeddings.npy  (L2â€‘normalised)
â”‚   â€¢ run HDBSCAN(min_cluster_size=2)
â”‚   â€¢ output cluster_map.json  {filename: cluster_id}
â”‚
â””â”€â”€ evaluate.py           # 4.  Quality checks
    â€¢ compute Silhouette & Daviesâ€‘Bouldin indices
    â€¢ plot clusterâ€‘size histogram
    â€¢ list recordings with low silhouette for manual review
```

---

## Challenges & Mitigations

| Challenge                 | Mitigation                                                                          |
| ------------------------- | ----------------------------------------------------------------------------------- |
| Similarâ€‘sounding speakers | Highâ€‘resolution ECAPA embeddings; iterative reâ€‘clustering of suspect large clusters |
| Noise / channel mismatch  | VAD + light spectral denoising; robustness of ECAPA (trained with augmentation)     |
| No ground truth           | Internal metrics; targeted manual listening; verificationâ€‘score crossâ€‘checks        |
| Overâ€‘ / underâ€‘clustering  | Compare HDBSCAN vs. fixedâ€‘k AHC, inspect cluster size distribution                  |


---

## Repository Layout

```
Exploring-Unlabelled-Speaker-Recognition/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # <â€‘â€‘ 200 original WAVs go here
â”‚   â”œâ”€â”€ clean/        # autoâ€‘generated VADâ€‘trimmed clips
â”‚   â””â”€â”€ embeddings/   # autoâ€‘generated .npy vectors
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ cluster.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ Exploring Unlabelled Speaker Recognition Documentation.pdf   # <â€‘â€‘ Detailed writeâ€‘up
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---

## Quickâ€‘Start (Conceptual)

```bash
# 1. Prepare env
python3.12 -m venv AINgineer && source AINgineer/bin/activate  # Windows: AINgineer\Scripts\activate
pip install -r requirements.txt

# 2. Run pipeline
python src/preprocess.py   # cleans /data/*.wav â†’ /data/clean/*.wav
python src/embed.py        # â†’ embeddings.npy
python src/cluster.py      # â†’ cluster_map.json
python src/evaluate.py     # prints silhouette & saves plots
```

---

## ğŸ“Š Clustering Evaluation Results

| Algorithm                                            | SilhouetteÂ Score<sup>â€ </sup> | Daviesâ€‘BouldinÂ Index<sup>â€¡</sup> | Observations                                                                                                       |
| ---------------------------------------------------- | ---------------------------- | -------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **HDBSCAN**                                          | **0.510**                    | **0.824**                        | Same overall quality as Kâ€‘Means on this dataset, while automatically discovering cluster count and flagging noise. |
| **Kâ€‘Means** (kâ€¯=â€¯60, n\_initâ€¯=â€¯20, seedâ€¯=â€¯42)        | **0.510**                    | **0.824**                        | Fast and simple; matched HDBSCANâ€™s scores but required preâ€‘setting *k* and forces every point into a cluster.      |
| **SpectralÂ Clustering** (nearestâ€‘neighbors affinity) | âˆ’0.024                       | 2.614                            | Performed poorly â€” negative silhouette and high DB index indicate illâ€‘formed clusters for this embedding.          |

<sup>â€ </sup> **Silhouette score** âˆˆâ€¯\[âˆ’1,â€¯1] Â â€¢Â Â +1â€¯=â€¯wellâ€‘separated, Â 0â€¯â‰ˆâ€¯overlap, Â âˆ’1â€¯=â€¯misâ€‘clustered. Higher is better. <sup>â€¡</sup> **Daviesâ€‘Bouldin index**Â â‰¥â€¯0 Â â€¢Â Â 0â€¯=â€¯perfectly compact/isolated clusters. Lower is better.

---

## ğŸ§ About the Dataset â€” *AudioMNIST (Combined)*

| Property                | Value                                       |
| ----------------------- | ------------------------------------------- |
| **Total clips**         | 30â€¯000 WAV files                            |
| **Speakers**            | 60 (one folder per speaker)                 |
| **Digits**              | 0â€¯â€“â€¯9 (spoken)                              |
| **Samples per speaker** | 500 (â‰ˆÂ 50 clips per digit before combining) |
| **Metadata**            | `audioMNIST_meta.txt` (gender, age, etc.)   |
| **Source**              | Kaggle dataset â€œAudio MNISTâ€        |

For this task the raw digit recordings for each speaker were **concatenated digitâ€‘wise** (e.g., all â€œ0â€â€¯â†’â€¯`00_combined.wav`, all â€œ1â€â€¯â†’â€¯`01_combined.wav`, â€¦) to create longer utterances, yielding exactly **60â€¯Ã—â€¯50 = 3â€¯000 combined clips** used in the clustering experiments.

---

### ğŸ“ˆ Metric Ranges & Why They Matter

* **Silhouette Score (S)**

  * Range **âˆ’1â€¯â†’â€¯1**.
  * *Interpretation*: Sâ€¯â‰³â€¯0.5 is generally considered good; Sâ€¯<â€¯0 suggests points are assigned to the wrong clusters.

* **Daviesâ€‘Bouldin Index (DBI)**

  * Range **0â€¯â†’â€¯âˆ**.
  * *Interpretation*: DBIâ€¯â‰²â€¯1 indicates compact, wellâ€‘separated clusters; valuesâ€¯â‰«â€¯1 mean high overlap.

These complementary metrics help avoid relying on a single view of cluster quality.

---

### ğŸ” Quick Takeaways

* **HDBSCAN** matched Kâ€‘Meansâ€™ quantitative scores **without** requiring you to guess *k* and **flagged noise points** automatically â€” valuable for speakerâ€‘embedding spaces that may contain outliers.
* **Spectral Clustering** underâ€‘performed on the same embeddings, suggesting either a poor affinity choice or that the speaker manifold is not well captured by a graphâ€‘based approach here.
* Given equal scores, you might prefer **HDBSCAN** for its flexibility and practical benefits (automatic cluster count, noise handling).

---

*Project bootstrapped May 2025 â€“ innovation in progress ğŸš§*
