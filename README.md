# Exploring-Unlabelled-Speaker-Recognition

> **Refer to the full documentation (`Exploring Unlabelled Speaker Recognition Documentation.pdf`) if you need more detailed explanations.**

---

## 1 Overview

This repository proposes and outlines a **fullyâ€‘unsupervised, highâ€‘accuracy pipeline** for identifying *â‰ˆ200* unique speakers in an unlabeled collection of WAV recordings.
The core idea:

1. **Preâ€‘process** audio (resample, VAD, normalise).
2. **Embed** each utterance with a **preâ€‘trained ECAPAâ€‘TDNN** (or xâ€‘vector) network.
3. **Cluster** the embedding vectors (HDBSCANâ€¯/â€¯Spectral Clustering) to form speaker IDs.
4. **Validate** clusters using internal metrics & manual spotâ€‘checks.

Most of the heavy lifting is done by the powerful speakerâ€‘embedding model, allowing accurate separation without labelled data.

---

## 2 Data Exploration & Analysis

| **Exploration Step**                 | **Purpose**                                                                                         |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- |
| Listening to audio samples           | Understand voice diversity, accents, noise, and potential overlap between speakers                  |
| Plotting waveform diagrams           | Identify amplitude variations, silence regions, clipping, or speech rhythm anomalies                |
| Spectrogram visualization            | Observe pitch, formant structures, and frequency distribution across time                           |
| Statistical feature extraction       | Analyze MFCCs, pitch, and speaking rate to detect broad groupings (e.g., by gender or vocal traits) |
| Silence and speech segmentation      | Detect and isolate active voice regions using VAD; flag recordings with multiple speakers           |
| Feature space projection (PCA/t-SNE) | Visualize embedding clusters to assess speaker separability and clustering potential                |


Challenges discovered:

* Varying recording quality & background noise.
* Possible nearâ€‘identical voices.
* No groundâ€‘truth labels for evaluation.

---

## 3 Proposed Solution & Justification

| **Component**            | **Selected Method**                                                     | **Justification**                                                                       |
| ------------------------ | ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Speaker Embeddings**   | ECAPAâ€‘TDNN (192â€‘D)                                                      | State-of-the-art model with strong robustness to noise and speaker variability          |
| **Similarity Metric**    | Cosine similarity                                                       | Scale-invariant; matches the training objective of modern speaker embeddings            |
| **Clustering Algorithm** | HDBSCAN or Spectral Clustering                                          | Handles variable cluster sizes; identifies outliers; no need to predefine cluster count |
| **Evaluation Strategy**  | Silhouette score, Davies-Bouldin Index (DBI), manual audio verification | Effective for unsupervised validation when no ground truth is available                 |


Why it works: embeddings compress speaker identity into a compact vector; clustering then groups vectors that are naturally close in this space.

---

## 4 Conceptual Implementation Strategy

```text
src/
â”œâ”€â”€ preprocess.py         # 1.  Audio cleaning
â”‚   â€¢ resample to 16â€¯kHz mono
â”‚   â€¢ apply VAD â†’ trim silence / noise
â”‚   â€¢ loudnessâ€‘normalise â†’ data/clean/
â”‚
â”œâ”€â”€ embed.py              # 2.  Speakerâ€‘vector extraction
â”‚   â€¢ load SpeechBrain ECAPAâ€‘TDNN
â”‚   â€¢ each WAV â†’ 192â€‘D embedding
â”‚   â€¢ stack â†’ embeddings.npy  +  filenames.txt
â”‚
â”œâ”€â”€ cluster.py            # 3.  Unsupervised grouping
â”‚   â€¢ load embeddings.npy  (L2â€‘normalised)
â”‚   â€¢ run HDBSCAN(min_cluster_size=2)
â”‚   â€¢ output cluster_map.json  {filename: cluster_id}
â”‚
â””â”€â”€ evaluate.py           # 4.  Quality checks
    â€¢ compute Silhouette & Daviesâ€‘Bouldin indices
    â€¢ plot clusterâ€‘size histogram
    â€¢ list recordings with low silhouette for manual review
```

All code is **conceptual/pseudocode** and can be converted to runnable Python with minimal effort.

---

## 5 Challenges & Mitigations

| Challenge                 | Mitigation                                                                          |
| ------------------------- | ----------------------------------------------------------------------------------- |
| Similarâ€‘sounding speakers | Highâ€‘resolution ECAPA embeddings; iterative reâ€‘clustering of suspect large clusters |
| Noise / channel mismatch  | VAD + light spectral denoising; robustness of ECAPA (trained with augmentation)     |
| No ground truth           | Internal metrics; targeted manual listening; verificationâ€‘score crossâ€‘checks        |
| Overâ€‘ / underâ€‘clustering  | Compare HDBSCAN vs. fixedâ€‘k AHC, inspect cluster size distribution                  |


---

## 6 Repository Layout

```
Exploring-Unlabelled-Speaker-Recognition/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # <â€‘â€‘ 200 original WAVs go here
â”‚   â”œâ”€â”€ clean/        # autoâ€‘generated VADâ€‘trimmed clips
â”‚   â””â”€â”€ embeddings/   # autoâ€‘generated .npy vectors
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ cluster.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ Exploring Unlabelled Speaker Recognition Documentation.pdf   # <â€‘â€‘ Detailed writeâ€‘up
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---

## 7 Quickâ€‘Start (Conceptual)

```bash
# 1. Prepare env
python -m venv AINgineer && source AINgineer/bin/activate  # Windows: AINgineer\Scripts\activate
pip install -r requirements.txt

# 2. Run pipeline
python src/preprocess.py   # cleans /data/*.wav â†’ /data/clean/*.wav
python src/embed.py        # â†’ embeddings.npy
python src/cluster.py      # â†’ cluster_map.json
python src/evaluate.py     # prints silhouette & saves plots
```

---

## 8 Next Steps

1. Swap ECAPA for any newer embedding model if desired.
2. Iterate clustering parameters to hit exactly 200 clusters.
3. Build a small web dashboard to audition clusters and merge/split interactively.

---

*Project bootstrapped May 2025 â€“ innovation in progress ðŸš§*
